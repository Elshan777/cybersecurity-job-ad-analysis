{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4192f3-4cff-4f36-9def-177bbe0f2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8bfe2a-d092-4c0c-b628-e1be188016d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT</td>\n",
       "      <td>Inform</td>\n",
       "      <td>https://www.linkedin.com/company/global-blue/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT</td>\n",
       "      <td>Architect</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/2589036509/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT</td>\n",
       "      <td>Manager</td>\n",
       "      <td>www.linkedin.com/jobs/view/2540581439/\\r\\n\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/2997674064\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3020920801\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country      title                                               text\n",
       "0      AT     Inform  https://www.linkedin.com/company/global-blue/l...\n",
       "1      AT  Architect  https://www.linkedin.com/jobs/view/2589036509/...\n",
       "2      AT    Manager  www.linkedin.com/jobs/view/2540581439/\\r\\n\\r\\n...\n",
       "3      AT    Analyst  https://www.linkedin.com/jobs/view/2997674064\\...\n",
       "4      AT   Engineer  https://www.linkedin.com/jobs/view/3020920801\\..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/jobs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299f219c-defe-48d1-a00d-af3e7d5bb0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT</td>\n",
       "      <td>Inform</td>\n",
       "      <td>information technology security manager global...</td>\n",
       "      <td>Information Technology Security Manager\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT</td>\n",
       "      <td>Architect</td>\n",
       "      <td>information security architect copmany swarovs...</td>\n",
       "      <td>Information Security Architect\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT</td>\n",
       "      <td>Manager</td>\n",
       "      <td>information technology security manager copman...</td>\n",
       "      <td>Information Technology Security Manager\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>threat detection analyst company radar cyber s...</td>\n",
       "      <td>Threat Detection Analyst\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>security engineer company global blue location...</td>\n",
       "      <td>IT Security Engineer\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country      title                                               text  \\\n",
       "0      AT     Inform  information technology security manager global...   \n",
       "1      AT  Architect  information security architect copmany swarovs...   \n",
       "2      AT    Manager  information technology security manager copman...   \n",
       "3      AT    Analyst  threat detection analyst company radar cyber s...   \n",
       "4      AT   Engineer  security engineer company global blue location...   \n",
       "\n",
       "                                   job_title  \n",
       "0  Information Technology Security Manager\\r  \n",
       "1           Information Security Architect\\r  \n",
       "2  Information Technology Security Manager\\r  \n",
       "3                 Threat Detection Analyst\\r  \n",
       "4                     IT Security Engineer\\r  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of links\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
    "\n",
    "# Extract Job Title from text\n",
    "df['job_title'] = df['text'].apply(lambda x: (re.sub(r'^.*?\\n', '\\n', x).strip()).split('\\n')[0] )\n",
    "\n",
    "\n",
    "def remove_Stopwords(text ):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    words = word_tokenize( text.lower() ) \n",
    "    sentence = [w for w in words if not w in stop_words]\n",
    "    return \" \".join(sentence)\n",
    "    \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    wordlist=[]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    sentences=sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            wordlist.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(wordlist) \n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr]) \n",
    "    \n",
    "    return text2.lower()\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].apply(remove_Stopwords)\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8db0571-5a36-43d7-940a-5c2e5fa7c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TextBlob(df['text'][0]).ngrams(2)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9472327b-82a0-49e7-9448-f21ec2039ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile():\n",
    "    \"\"\"\n",
    "    This function will read the text files passed & return the list\n",
    "    \"\"\"\n",
    "    # fileObj = open(fileName, \"r\") #opens the file in read mode\n",
    "    words = df['text'][0].splitlines() #puts the file into a list\n",
    "    # fileObj.close()\n",
    "    return words\n",
    "\n",
    "def read_nGrams():\n",
    "    \"\"\"\n",
    "    This function will read bigrams & trigrams and \n",
    "    return combined list of bigrams & trigrams.\n",
    "    \"\"\"\n",
    "    # read  bigrams \n",
    "    original_bigram = readFile()\n",
    "    # read trigrams\n",
    "    # original_trigram = readFile(\"trigram.txt\")\n",
    "\n",
    "    # Combined list of bigrams & trigrams\n",
    "    # n_grams_to_use = []\n",
    "    # n_grams_to_use.extend(original_bigram)\n",
    "    # n_grams_to_use.extend(original_trigram)\n",
    "    # return n_grams_to_use\n",
    "    \n",
    "    return original_bigram\n",
    "n_grams_to_use = read_nGrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9113e0b5-7f28-41eb-8c03-908f1828b9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each n-gram into separate words\n",
    "def split_nGrams(n_grams_to_use):\n",
    "    ngrams_splited = [each.split() for each in n_grams_to_use]\n",
    "    return ngrams_splited\n",
    "ngrams_splited = split_nGrams(n_grams_to_use)\n",
    "len(ngrams_splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e28cba-f83f-4c34-906c-aae89a6efa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(list_words, model, vocabulary, num_features):\n",
    "    \"\"\"\n",
    "    This function will take each tokenized sentence having bigrams or trigrams, \n",
    "    model = the mapping_of_word_to_vector dictionary, vocabulary = unique set of keys(words) present in model,\n",
    "    num_features = 50\n",
    "    \n",
    "    This function will return the average of feature vector for each word present in list_words.\n",
    "    \"\"\"\n",
    "    # Created array of zeros (type float) of size num_features, i.e., 50.\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    # Put it in try block so that if any exception occur, it will be dealt by below exception block.\n",
    "    try:\n",
    "        # Check if word is in passed list_of_words or not.\n",
    "        for word in list_words:\n",
    "            # Check if word is in general vocabulary or not (the unique set of words in word embedding).\n",
    "            if word in vocabulary: \n",
    "                # Increment number_of_words\n",
    "                nwords = nwords + 1\n",
    "                # add vector array of corresponding key in model which matches the passed word.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "        if nwords:\n",
    "            # Take average of feature_vector by dividing with total number of words\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "    except:\n",
    "        # If the exception occurs, while the word isn't found in vocabulary, it will return the array of zeros\n",
    "        return np.zeros((num_features,),dtype=\"float64\")\n",
    "    \n",
    "\n",
    "    \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    \"\"\"\n",
    "    This function is taking corpus of bigrams & trigrams, w2v mappings, num of features as a input arguments.\n",
    "    and returning array of features after taking average using average_word_vectors() function.\n",
    "    \"\"\"\n",
    "    # Get the unique keys out of word_to_vector_map dictionary.\n",
    "    vocabulary = set(model.keys())\n",
    "    # Call function average_word_vectors which is returning with averaged vectors for each word in tokenized sentence.\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in ngrams_splited]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ca089-2ade-4fec-95c1-c758b67dc04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6861cc60-fee4-4a39-86d6-b4263a715966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(glove_path):\n",
    "    \"\"\"\n",
    "    This function will read glove data from text file and do the following:\n",
    "    1. prepare dictionary of words and vectors\n",
    "    2. prepare dictionary of words and index\n",
    "    3. prepare dictionary of index and words\n",
    "    \"\"\"\n",
    "    # Read word_embedding file stored on glove_path specified.\n",
    "    with open(glove_path, 'r', encoding='utf-8')as inp_file:\n",
    "        \n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        # For every line in embedding file which contains the word & the corresponding vector.\n",
    "        for line in inp_file:\n",
    "            # convert each line in embedding file to a list of elements.\n",
    "            line = line.strip().split()\n",
    "            # Get first element of the list, i.e., word of each list.\n",
    "            curr_word = line[0]\n",
    "            # Add the distinct set of words.\n",
    "            words.add(curr_word)\n",
    "            # Create dictionary that will map current word to that of it's vector representation.\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "        i=1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        # For every word in sorted dictionary of words\n",
    "        for w in sorted(words):\n",
    "            # map index to each words\n",
    "            words_to_index[w]=i\n",
    "            # map words to each index\n",
    "            index_to_words[i]=w\n",
    "            i += 1\n",
    "        \n",
    "        return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24de5674-9310-4d03-a40f-31071e6e879b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Generating_nGrams\\\\Text Clustering\\\\domain_embeddings.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14328/301215994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load glove vectors from pre-trained model domain dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"Generating_nGrams\\Text Clustering\\domain_embeddings.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_words_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index_to_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_word_to_vec_map\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mread_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14328/3099855968.py\u001b[0m in \u001b[0;36mread_glove\u001b[1;34m(glove_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Read word_embedding file stored on glove_path specified.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mas\u001b[0m \u001b[0minp_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Generating_nGrams\\\\Text Clustering\\\\domain_embeddings.txt'"
     ]
    }
   ],
   "source": [
    "# load glove vectors from pre-trained model domain dataset\n",
    "glove_path = r\"Generating_nGrams\\Text Clustering\\domain_embeddings.txt\"\n",
    "new_words_to_index, new_index_to_words, new_word_to_vec_map  = read_glove(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb39166-1070-4dfa-b3d4-13840df12c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
